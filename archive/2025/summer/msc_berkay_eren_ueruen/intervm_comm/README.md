# Interprocess Communication
This setup consists of two applications. The server, responsible of processing inference requests by making required llama.cpp calls and the client itself.

## Shared Memory
Inter-VM communication requires some preparation. First of all, the VMs must be started with some extra parameters. An example of this can be found in `benchmark/vm_server/server.nix` or `benchmark/vm_client/client.nix` files.

`-device ivshmem-plain,memdev=shmTest,bus=pci.0,addr=0x12,master=on`

This first parameters adds a shared memory device. `addr` field here can be anything, however, as it is used by the source code in `shm.h` file, it must be consistent with that.

`-object memory-backend-file,size=32M,share=on,mem-path=/dev/shm/shmTest,id=shmTest`

This second parameter specifies the backend required by the shared memory device. As long as the same name, in this case `shmTest`, is consistently used, only interesting parameter is the size, which can be set according to needs. 32M here is selected without any particular meaning.

When a QEMU VM is started with theses parameters, it will have an access to the defined shared memory. When two QEMU VMs are started with the exact same shared memory parameters, both will have access to the same shared memory.

The resulting memory is represented as a file. In the hypervisor, it can be found in `/dev/shm`. In VMs, its location depends on the `addr` parameter that we specified. However, it will be under the `/sys/bus/pci/devices/` directory. With our example parameters, the exact locations looks like this: `/sys/bus/pci/devices/0000:00:12.0/resource2`.

In source code, this shared memory file must be opened and then `mmap`ed. An example can be found in `/intervm_comm/src/shm.cpp`.

### Native Testing
In case the inter-VM setup will be run natively without any VMs. It is possible to create a shared memory by simply running `touch /dev/shm/shmTest`. `shmTest` here can be any name imaginable. Similar to VM setup, `shm.h` file must be updated so that it will use this local shared memory file.

### Troubleshooting
In some cases, especially when the hypervisor mistakenly writes to shared memory. QEMU complaints about the shared memory while starting a VM. The best way to overcome any errors is simply deleting the shared memory file in `/dev/shm` folder. This is safe, as long as the data in the file is expendable, as the file will be automatically created again while launching a QEMU VM.

## Compilation
Intervm communication makes use of shared libraries generated by the llama.cpp compilation process. Therefore we first need to compile the libraries.

### llama.cpp Library Compilation
Using `cmake` directly generates libraries inside of `llama.cpp/build/bin`. 
```
# In the root of llama.cpp
cmake -B build -DGGML_CUDA=ON -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

`make` on the other hand must be called with the following command, however with the latest version it is deprecated:
```
# In the root of llama.cpp
make lib*
```
This will generate libraries in the root directory `llama.cpp`.

If this fails to generate lib files, each of them should be generated manually
```
# In the root of llama.cpp
make libggml.so
make libggml.a
make libcommon.so
make libcommon.a
make libllama.so
make libllama.a
```

### Intervm Communication Compilation

```
nix-shell ../cuda-shell.nix
make
```
As a result a server and a client binary will be created.

## Usage
A good way to test the setup is launching two VMs, one of them dedicated to be the server and the other to be the client. The instructions for that can be found on the root README.

On the client VM side, by using `tmux` or `screen`, it is possible to create multiple panels to follow multiple clients at the same time.

### Server
Only mandatory option the server takes is the path to the model file. It is also possible to specify LoRA file(s).
```
Usage: server [OPTION...]
LLM-OS Server

  -l, --lora=FILE            Path to a LoRA file (Can be specified multiple
                             times)
  -m, --model=FILE           Path to the model file (Required)
  -?, --help                 Give this help list
      --usage                Give a short usage message
```
### Client
Client requires an `id` to be specified. This `id` currently also selects the prompt to be send. `lora_number` and `prio` are optional extra arguments that default to 0 when not specified.
```
Usage: client [OPTION...]
Client-side argument parser

  -i, --id=ID                User ID (Required)
  -n, --lora_number=NUM      Number of LoRA (Default = 0)
  -p, --prio=PRIO            Priority level (Default = 0)
  -?, --help                 Give this help list
      --usage                Give a short usage message
```
